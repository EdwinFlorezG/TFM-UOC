{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install EMD-signal antropy --quiet"
      ],
      "metadata": {
        "id": "gqWlRUFapncY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üîß Importaci√≥n de librer√≠as\n",
        "\n",
        "En esta celda se importan todas las librer√≠as necesarias para el desarrollo del proyecto. Estas se agrupan por funcionalidad:\n",
        "\n",
        "- **Librer√≠as est√°ndar:** manejo de archivos, advertencias, arrays y dataframes.\n",
        "- **An√°lisis estad√≠stico y modelado ARIMA.**\n",
        "- **An√°lisis de se√±ales:** descomposici√≥n EEMD y c√°lculo de entrop√≠a por permutaci√≥n.\n",
        "- **Preprocesamiento y m√©tricas.**\n",
        "- **Modelado con redes neuronales LSTM usando Keras.**\n",
        "- **Carga de archivos desde Google Drive (entornos Colab).**\n",
        "- **Manejo de fechas y advertencias.**\n"
      ],
      "metadata": {
        "id": "PWXh_0qZiSGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# Importaci√≥n de librer√≠as\n",
        "# ============================\n",
        "\n",
        "# Librer√≠as est√°ndar\n",
        "import os                        # Para operaciones del sistema de archivos\n",
        "import pickle                   # Para serializaci√≥n de objetos en disco\n",
        "import warnings                 # Para controlar la visualizaci√≥n de advertencias\n",
        "import numpy as np              # Operaciones con arrays num√©ricos\n",
        "import pandas as pd             # Manipulaci√≥n de datos tabulares\n",
        "import matplotlib.pyplot as plt # Visualizaci√≥n de gr√°ficos\n",
        "import seaborn as sns           # Visualizaciones estad√≠sticas avanzadas\n",
        "\n",
        "# Librer√≠as estad√≠sticas\n",
        "from scipy.stats import linregress                         # Regresi√≥n lineal simple\n",
        "from statsmodels.tsa.arima.model import ARIMA              # Modelado ARIMA\n",
        "from statsmodels.tools.sm_exceptions import ConvergenceWarning  # Advertencia espec√≠fica de statsmodels\n",
        "\n",
        "# An√°lisis de se√±ales\n",
        "from PyEMD import EEMD               # Descomposici√≥n emp√≠rica con modo ensemble\n",
        "from antropy import perm_entropy    # C√°lculo de entrop√≠a por permutaci√≥n\n",
        "\n",
        "# Preprocesamiento y evaluaci√≥n\n",
        "from sklearn.preprocessing import MinMaxScaler                 # Normalizaci√≥n de datos\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score  # M√©tricas de evaluaci√≥n\n",
        "\n",
        "# Modelado LSTM con Keras\n",
        "from tensorflow.keras.models import Sequential, load_model    # Creaci√≥n y carga de modelos\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Input  # Capas para redes neuronales LSTM\n",
        "\n",
        "# Colab y visualizaci√≥n interactiva\n",
        "from google.colab import drive             # Acceso a Google Drive\n",
        "from IPython.display import display        # Visualizaci√≥n mejorada en notebooks\n",
        "\n",
        "# Manipulaci√≥n de fechas\n",
        "from dateutil.relativedelta import relativedelta  # Operaciones con fechas relativas\n",
        "\n",
        "# Suprimir advertencias de convergencia de ARIMA\n",
        "warnings.simplefilter(\"ignore\", ConvergenceWarning)\n"
      ],
      "metadata": {
        "id": "Cwm9mSlmiSVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìÅ Configuraci√≥n de rutas y estructura de carpetas\n",
        "\n",
        "Esta celda monta Google Drive (en entornos como Google Colab) y configura la estructura de carpetas del proyecto:\n",
        "\n",
        "- Se define la ruta base del proyecto (`BASE_DIR`).\n",
        "- Se especifica la ubicaci√≥n del archivo de datos CSV.\n",
        "- Se crea un diccionario `DIRS` con rutas organizadas para almacenar:\n",
        "  - Datos originales.\n",
        "  - Modelos entrenados (LSTM y ARIMA).\n",
        "  - Resultados estad√≠sticos de entrenamiento y prueba.\n",
        "  - Gr√°ficas de an√°lisis exploratorio, EEMD y evaluaci√≥n de modelos.\n",
        "  - Predicciones futuras.\n",
        "\n",
        "Finalmente, se asegura que todas las carpetas existan mediante `os.makedirs(...)`.\n"
      ],
      "metadata": {
        "id": "jXotVm2XkNfg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================\n",
        "# Montaje de Google Drive y configuraci√≥n inicial\n",
        "# ===============================================\n",
        "\n",
        "# Montar Google Drive para acceder a archivos almacenados en la nube\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Ruta base del proyecto (puedes modificarla seg√∫n tu estructura)\n",
        "BASE_DIR = \"/content/drive/MyDrive/\"\n",
        "\n",
        "# Ruta al archivo CSV con los datos de precipitaci√≥n\n",
        "DATA_PATH = os.path.join(BASE_DIR, \"datos\", \"descargaDhime.csv\")\n",
        "\n",
        "# Diccionario de rutas para organizar los diferentes productos del proyecto\n",
        "DIRS = {\n",
        "    # Carpeta con los datos originales\n",
        "    \"datos\": f\"{BASE_DIR}/datos\",\n",
        "\n",
        "    # Modelos entrenados\n",
        "    \"lstm_models\": f\"{BASE_DIR}/modelos/lstm\",         # Modelos LSTM por estaci√≥n y componente\n",
        "    \"arima_models\": f\"{BASE_DIR}/modelos/arima\",       # Modelos ARIMA por estaci√≥n y componente\n",
        "\n",
        "    # Resultados estad√≠sticos de entrenamiento y prueba\n",
        "    \"stats_train\": f\"{BASE_DIR}/estadisticas/eemd/train\",\n",
        "    \"stats_test\": f\"{BASE_DIR}/estadisticas/eemd/test\",\n",
        "\n",
        "    # Gr√°ficas del an√°lisis exploratorio y EEMD\n",
        "    \"graficos_precipitacion\": f\"{BASE_DIR}/graficas/precipitacion\",\n",
        "    \"graficos_eda\": f\"{BASE_DIR}/graficas/exploratorio\",\n",
        "    \"graficos_eemd\": f\"{BASE_DIR}/graficas/eemd\",\n",
        "\n",
        "    # Gr√°ficas de evaluaci√≥n en entrenamiento\n",
        "    \"graficos_train_mse\": f\"{BASE_DIR}/graficas/resultados_modelo/train/curvas_mse\",\n",
        "    \"graficos_train_dispersion\": f\"{BASE_DIR}/graficas/resultados_modelo/train/dispersion\",\n",
        "    \"graficos_train_reconstruccion\": f\"{BASE_DIR}/graficas/resultados_modelo/train/reconstruccion\",\n",
        "\n",
        "    # Gr√°ficas de evaluaci√≥n en validaci√≥n (test)\n",
        "    \"graficos_test_dispersion\": f\"{BASE_DIR}/graficas/resultados_modelo/test/dispersion\",\n",
        "    \"graficos_test_reconstruccion\": f\"{BASE_DIR}/graficas/resultados_modelo/test/reconstruccion\",\n",
        "\n",
        "    # Predicciones a futuro generadas por estaci√≥n\n",
        "    \"predicciones_futuras\": f\"{BASE_DIR}/predicciones/futuro\"\n",
        "}\n",
        "\n",
        "# Crear todas las carpetas necesarias si a√∫n no existen\n",
        "for ruta in DIRS.values():\n",
        "    os.makedirs(ruta, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "TADHooMSkQur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üßæ Carga de datos\n",
        "\n",
        "Se define una funci√≥n `cargar_datos(path)` que:\n",
        "\n",
        "- Lee un archivo CSV desde la ruta especificada.\n",
        "- Convierte la columna `'Fecha'` al tipo `datetime`, manejando errores silenciosamente.\n",
        "- Devuelve un `DataFrame` con los datos listos para ser analizados.\n",
        "\n",
        "Luego, se utiliza esta funci√≥n para cargar los datos de precipitaci√≥n almacenados en la ruta `DATA_PATH`.\n"
      ],
      "metadata": {
        "id": "tb3tFb3DkTtf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================\n",
        "# Funci√≥n para cargar y procesar datos\n",
        "# ===================================\n",
        "\n",
        "def cargar_datos(path):\n",
        "    \"\"\"\n",
        "    Carga un archivo CSV y convierte la columna 'Fecha' a formato datetime.\n",
        "\n",
        "    Par√°metros:\n",
        "        path (str): Ruta al archivo CSV.\n",
        "\n",
        "    Retorna:\n",
        "        pd.DataFrame: Datos cargados y preparados.\n",
        "    \"\"\"\n",
        "    print(\"üì• Cargando archivo...\")\n",
        "    # Lee el archivo CSV usando codificaci√≥n latin-1 para evitar errores con caracteres especiales\n",
        "    df = pd.read_csv(path, encoding='latin-1')\n",
        "\n",
        "    # Convierte la columna 'Fecha' a tipo datetime, ignorando errores\n",
        "    df['Fecha'] = pd.to_datetime(df['Fecha'], errors='coerce')\n",
        "\n",
        "    print(f\"‚úÖ Datos cargados: {df.shape[0]} registros\")\n",
        "    return df\n",
        "\n",
        "# Carga del archivo principal de datos de precipitaci√≥n\n",
        "df = cargar_datos(DATA_PATH)\n"
      ],
      "metadata": {
        "id": "LYS7r0Cxkl3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üßº Interpolaci√≥n y segmentaci√≥n por estaci√≥n\n",
        "\n",
        "En esta celda se define la funci√≥n `interpolar_y_separar_por_estacion(df)`, que:\n",
        "\n",
        "- Limpia los nombres de las estaciones para unificar formatos y eliminar etiquetas.\n",
        "- Interpola los valores faltantes (NaN) por estaci√≥n utilizando interpolaci√≥n lineal.\n",
        "- Genera un `DataFrame` por estaci√≥n con frecuencia mensual (`'MS'`).\n",
        "- Retorna un diccionario donde cada clave es una estaci√≥n y el valor es su respectivo `DataFrame` ordenado por fecha.\n",
        "\n",
        "Esto permite trabajar de manera individual con cada serie temporal de precipitaci√≥n.\n"
      ],
      "metadata": {
        "id": "6VaNjIV8k9IK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# Funci√≥n para interpolar y separar datos por estaci√≥n\n",
        "# =====================================================\n",
        "\n",
        "def interpolar_y_separar_por_estacion(df):\n",
        "    \"\"\"\n",
        "    Interpola los valores faltantes por estaci√≥n y separa los datos en un diccionario.\n",
        "\n",
        "    Tambi√©n realiza limpieza de nombres de estaciones.\n",
        "\n",
        "    Par√°metros:\n",
        "        df (pd.DataFrame): Datos crudos con columnas 'Fecha' y 'NombreEstacion'.\n",
        "\n",
        "    Retorna:\n",
        "        dict: Diccionario con DataFrames por estaci√≥n.\n",
        "    \"\"\"\n",
        "    # Limpieza de nombres de estaci√≥n eliminando etiquetas al final como [M], [A], etc.\n",
        "    df[\"NombreEstacion\"] = df[\"NombreEstacion\"].astype(str).str.replace(r'\\s*\\[.*\\]$', '', regex=True)\n",
        "\n",
        "    # Reemplazos manuales para homogeneizar nombres\n",
        "    reemplazos = {\n",
        "        \"CUCHARO EL\": \"EL CUCHARO\",\n",
        "        \"MAMONAL EL HACIENDA\": \"HACIENDA EL MAMONAL\",\n",
        "        \"PAVAS LAS\": \"LAS PAVAS\",\n",
        "        # Agrega m√°s casos seg√∫n sea necesario\n",
        "    }\n",
        "    df[\"NombreEstacion\"] = df[\"NombreEstacion\"].replace(reemplazos)\n",
        "\n",
        "    estaciones = df[\"NombreEstacion\"].dropna().unique()\n",
        "    df_total = []\n",
        "\n",
        "    for est in estaciones:\n",
        "        # Filtra los datos para la estaci√≥n actual\n",
        "        df_est = df[df[\"NombreEstacion\"] == est].copy()\n",
        "\n",
        "        # Genera un rango de fechas mensuales\n",
        "        fechas = pd.date_range(df_est['Fecha'].min(), df_est['Fecha'].max(), freq='MS')\n",
        "        df_fechas = pd.DataFrame({'Fecha': fechas})\n",
        "\n",
        "        # Une el rango de fechas con los datos reales\n",
        "        df_merge = pd.merge(df_fechas, df_est, on='Fecha', how='left')\n",
        "\n",
        "        # Interpola columnas num√©ricas que tengan valores faltantes\n",
        "        for col in df_merge.columns:\n",
        "            if col != \"Fecha\" and pd.api.types.is_numeric_dtype(df_merge[col]):\n",
        "                df_merge[col] = df_merge[col].interpolate(method='linear')\n",
        "\n",
        "        # Rellena nombre de estaci√≥n si qued√≥ en NaN\n",
        "        df_merge[\"NombreEstacion\"].fillna(est, inplace=True)\n",
        "\n",
        "        # Agrega el DataFrame procesado al conjunto total\n",
        "        df_total.append(df_merge)\n",
        "\n",
        "    # Concatenaci√≥n de todos los registros procesados\n",
        "    df_final = pd.concat(df_total, ignore_index=True)\n",
        "\n",
        "    # Crea un diccionario por estaci√≥n con los datos ordenados por fecha\n",
        "    return {\n",
        "        est: df_final[df_final[\"NombreEstacion\"] == est].copy().sort_values(\"Fecha\").reset_index(drop=True)\n",
        "        for est in estaciones\n",
        "    }\n",
        "\n",
        "# Aplicar funci√≥n a los datos cargados\n",
        "df_estaciones = interpolar_y_separar_por_estacion(df)\n",
        "print(\"‚úÖ Estaciones procesadas:\", sorted(df_estaciones.keys()))\n"
      ],
      "metadata": {
        "id": "eBnH7cRMk-Bd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìä Visualizaci√≥n de la precipitaci√≥n por estaci√≥n\n",
        "\n",
        "Esta celda define la funci√≥n `graficar_precipitacion(df_estaciones)`, la cual:\n",
        "\n",
        "- Genera gr√°ficos mensuales y anuales de precipitaci√≥n por estaci√≥n.\n",
        "- Aplica regresi√≥n lineal y medias m√≥viles para observar tendencias a largo plazo.\n",
        "- Guarda los gr√°ficos generados en la carpeta `graficos_precipitacion`.\n",
        "\n",
        "Cada estaci√≥n tendr√°:\n",
        "- Un gr√°fico mensual con:\n",
        "  - Precipitaci√≥n total por mes.\n",
        "  - Media m√≥vil de 12 meses.\n",
        "  - L√≠nea de tendencia (regresi√≥n lineal).\n",
        "- Un gr√°fico anual con:\n",
        "  - Suma anual de precipitaci√≥n.\n",
        "  - Media m√≥vil de 7 a√±os.\n",
        "  - Promedio hist√≥rico y l√≠nea de tendencia."
      ],
      "metadata": {
        "id": "sTHd7URClEih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ===================================================\n",
        "# Funci√≥n para graficar precipitaci√≥n mensual/anual\n",
        "# ===================================================\n",
        "\n",
        "def graficar_precipitacion(df_estaciones):\n",
        "    \"\"\"\n",
        "    Genera gr√°ficos mensuales y anuales por estaci√≥n.\n",
        "\n",
        "    Par√°metros:\n",
        "        df_estaciones (dict): Diccionario con DataFrames por estaci√≥n.\n",
        "    \"\"\"\n",
        "    # Asegura que el directorio de salida existe\n",
        "    os.makedirs(DIRS[\"graficos_precipitacion\"], exist_ok=True)\n",
        "\n",
        "    for station, station_df in df_estaciones.items():\n",
        "        # Limita los datos hasta el a√±o 2024\n",
        "        station_df = station_df[station_df['Fecha'].dt.year <= 2024]\n",
        "\n",
        "        # ====================\n",
        "        # Gr√°fico mensual\n",
        "        # ====================\n",
        "        monthly = station_df.set_index('Fecha')['Valor'].resample('M').sum()\n",
        "        if not monthly.empty:\n",
        "            rolling_12 = monthly.rolling(window=12).mean()  # Media m√≥vil de 12 meses\n",
        "            x = np.arange(len(monthly))\n",
        "            slope, intercept, *_ = linregress(x, monthly)\n",
        "            reg_line = intercept + slope * x\n",
        "\n",
        "            fig, ax = plt.subplots(figsize=(10, 5))\n",
        "            ax.plot(monthly.values, color='blue', label='Mensual')\n",
        "            ax.plot(rolling_12.values, '--', color='orange', label='Media m√≥vil (12m)')\n",
        "            ax.plot(reg_line, color='black', label='Regresi√≥n lineal')\n",
        "            ax.set_title(f\"Precipitaci√≥n mensual - {station}\")\n",
        "            ax.set_xlabel(\"Mes\")\n",
        "            ax.set_ylabel(\"Precipitaci√≥n (mm)\")\n",
        "            ax.legend()\n",
        "            ax.grid(True, linestyle='--', alpha=0.6)\n",
        "            plt.tight_layout()\n",
        "\n",
        "            filename = station.replace('/', '_').replace(' ', '_') + '_mensual.png'\n",
        "            plt.savefig(os.path.join(DIRS[\"graficos_precipitacion\"], filename), dpi=300)\n",
        "            plt.show()\n",
        "            plt.close(fig)\n",
        "\n",
        "        # ====================\n",
        "        # Gr√°fico anual\n",
        "        # ====================\n",
        "        annual = station_df.set_index('Fecha')['Valor'].resample('Y').sum()\n",
        "        if not annual.empty:\n",
        "            rolling_7 = annual.rolling(window=7).mean()  # Media m√≥vil de 7 a√±os\n",
        "            x_years = np.arange(len(annual))\n",
        "            slope_a, intercept_a, *_ = linregress(x_years, annual)\n",
        "            reg_line_a = intercept_a + slope_a * x_years\n",
        "            avg_line = [annual.mean()] * len(annual)\n",
        "\n",
        "            fig, ax = plt.subplots(figsize=(10, 5))\n",
        "            ax.plot(annual.values, color='blue', label='Anual')\n",
        "            ax.plot(rolling_7.values, '--', color='orange', label='Media m√≥vil (7a)')\n",
        "            ax.plot(reg_line_a, color='black', label='Regresi√≥n lineal')\n",
        "            ax.plot(avg_line, color='red', label='Promedio hist√≥rico')\n",
        "            ax.set_title(f\"Precipitaci√≥n anual - {station}\")\n",
        "            ax.set_xlabel(\"A√±o\")\n",
        "            ax.set_ylabel(\"Precipitaci√≥n (mm)\")\n",
        "            ax.legend()\n",
        "            ax.grid(True, linestyle='--', alpha=0.6)\n",
        "            plt.tight_layout()\n",
        "\n",
        "            filename = station.replace('/', '_').replace(' ', '_') + '_anual.png'\n",
        "            plt.savefig(os.path.join(DIRS[\"graficos_precipitacion\"], filename), dpi=300)\n",
        "            plt.show()\n",
        "            plt.close(fig)\n",
        "\n",
        "# Ejecutar la funci√≥n para graficar todas las estaciones\n",
        "graficar_precipitacion(df_estaciones)\n"
      ],
      "metadata": {
        "id": "g5bbGGnvlEzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üåÄ Descomposici√≥n EEMD y partici√≥n de datos\n",
        "\n",
        "Esta celda define la funci√≥n `aplicar_eemd(df_estaciones)`, que:\n",
        "\n",
        "- Aplica la t√©cnica EEMD (Empirical Ensemble Mode Decomposition) a la serie de precipitaci√≥n de cada estaci√≥n.\n",
        "- Divide los resultados en conjuntos de entrenamiento y prueba utilizando como fecha de corte el 1 de enero de 2020.\n",
        "- Almacena las siguientes estructuras por estaci√≥n:\n",
        "  - `IMFs`: componentes intr√≠nsecos (modo).\n",
        "  - `serie`: serie original completa.\n",
        "  - `IMFs_train`, `IMFs_test`: componentes para entrenamiento y prueba.\n",
        "  - `serie_train`, `serie_test`: serie original dividida.\n",
        "\n",
        "Adem√°s, genera gr√°ficos con los IMFs por estaci√≥n y marca visualmente la fecha de partici√≥n.\n"
      ],
      "metadata": {
        "id": "PAHRxF5llXI9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# Aplicar EEMD a todas las estaciones y segmentar datos\n",
        "# =====================================================\n",
        "\n",
        "def aplicar_eemd(df_estaciones):\n",
        "    \"\"\"\n",
        "    Aplica EEMD a cada estaci√≥n y divide en conjuntos de entrenamiento/prueba.\n",
        "\n",
        "    Par√°metros:\n",
        "        df_estaciones (dict): Diccionario de DataFrames por estaci√≥n.\n",
        "\n",
        "    Retorna:\n",
        "        dict: Resultados por estaci√≥n, incluyendo IMFs, fechas y series divididas.\n",
        "    \"\"\"\n",
        "    imfs_dict = {}\n",
        "\n",
        "    for est, df in df_estaciones.items():\n",
        "        # Agrega la serie mensual, rellenando valores faltantes con 0\n",
        "        serie = df.set_index('Fecha')['Valor'].resample('MS').sum().fillna(0)\n",
        "        if len(serie) == 0:\n",
        "            continue\n",
        "\n",
        "        # Aplica EEMD\n",
        "        eemd = EEMD()\n",
        "        IMFs = eemd.eemd(serie.values)\n",
        "        fechas = serie.index\n",
        "\n",
        "        # Define el √≠ndice de corte para dividir en train/test (enero 2020)\n",
        "        split_idx = np.argmin(np.abs(fechas - pd.Timestamp(\"2020-01-01\")))\n",
        "\n",
        "        # Almacena los resultados en el diccionario\n",
        "        imfs_dict[est] = {\n",
        "            'IMFs': IMFs,\n",
        "            'serie': serie.values,\n",
        "            'fechas': fechas,\n",
        "            'IMFs_train': IMFs[:, :split_idx],\n",
        "            'IMFs_test': IMFs[:, split_idx:],\n",
        "            'serie_train': serie.values[:split_idx],\n",
        "            'serie_test': serie.values[split_idx:]\n",
        "        }\n",
        "\n",
        "        # ============\n",
        "        # Gr√°fico EEMD\n",
        "        # ============\n",
        "        fig, axs = plt.subplots(IMFs.shape[0], 1, figsize=(12, 2 * IMFs.shape[0]), sharex=True)\n",
        "        for i, ax in enumerate(axs):\n",
        "            ax.plot(fechas, IMFs[i], color='blue')\n",
        "            ax.axvline(pd.Timestamp(\"2020-01-01\"), color='red', linestyle='--')\n",
        "            ax.set_ylabel(f'IMF {i+1}')\n",
        "            ax.grid(True)\n",
        "\n",
        "        fig.suptitle(f\"EEMD - {est}\", fontsize=14)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(DIRS[\"graficos_eemd\"], f\"{est.replace('/','_')}_EEMD.png\"))\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "    return imfs_dict\n",
        "\n",
        "# Aplicar EEMD a las estaciones y guardar resultados\n",
        "imfs_dict = aplicar_eemd(df_estaciones)\n"
      ],
      "metadata": {
        "id": "VYeXX5zUlRRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìà C√°lculo de estad√≠sticas y entrop√≠a de IMFs\n",
        "\n",
        "Esta celda define la funci√≥n `calcular_estadisticas(...)`, que:\n",
        "\n",
        "- Calcula estad√≠sticas b√°sicas (m√°ximo, m√≠nimo, media, desviaci√≥n est√°ndar) de:\n",
        "  - Cada IMF (modo intr√≠nseco).\n",
        "  - La serie original (\"Raw\").\n",
        "  - El residuo (diferencia entre la serie original y la suma de los IMFs).\n",
        "- Calcula la entrop√≠a por permutaci√≥n (Permutation Entropy, PE) normalizada para cada IMF y el residual.\n",
        "- Clasifica los IMFs como de **alta** o **baja frecuencia** en funci√≥n de la mediana de sus valores de entrop√≠a.\n",
        "- Guarda los resultados como archivos CSV por estaci√≥n en las carpetas correspondientes (`train` o `test`).\n",
        "- Finalmente, ejecuta esta funci√≥n para todos los conjuntos de entrenamiento y prueba.\n",
        "\n",
        "Esto proporciona una caracterizaci√≥n estad√≠stica y espectral √∫til para la selecci√≥n de componentes a modelar.\n"
      ],
      "metadata": {
        "id": "KfqcX28dlndh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================\n",
        "# Funci√≥n para calcular estad√≠sticas y entrop√≠a por permutaci√≥n\n",
        "# ==============================================================\n",
        "\n",
        "def calcular_estadisticas(IMFs, serie_original, estacion=None, modo='train'):\n",
        "    \"\"\"\n",
        "    Calcula estad√≠sticas y entrop√≠a por permutaci√≥n de IMFs, serie original y residual.\n",
        "\n",
        "    Par√°metros:\n",
        "        IMFs (np.ndarray): Matriz con los IMFs.\n",
        "        serie_original (np.ndarray): Serie temporal original.\n",
        "        estacion (str): Nombre de la estaci√≥n (opcional para guardar).\n",
        "        modo (str): 'train' o 'test' para definir ruta de guardado.\n",
        "\n",
        "    Retorna:\n",
        "        pd.DataFrame: Tabla con estad√≠sticas y clasificaci√≥n de frecuencia.\n",
        "    \"\"\"\n",
        "    df_stats = []\n",
        "\n",
        "    for i, imf in enumerate(IMFs):\n",
        "        pe = perm_entropy(imf, normalize=True)\n",
        "        df_stats.append({\n",
        "            \"IMF\": f\"IMF{i+1}\",\n",
        "            \"Max\": np.max(imf),\n",
        "            \"Min\": np.min(imf),\n",
        "            \"Mean\": np.mean(imf),\n",
        "            \"Std\": np.std(imf),\n",
        "            \"PE\": pe\n",
        "        })\n",
        "\n",
        "    # Calcula el residuo: diferencia entre la serie original y la suma de los IMFs\n",
        "    residual = serie_original - IMFs.sum(axis=0)\n",
        "\n",
        "    # Estad√≠sticas de la serie original (sin entrop√≠a)\n",
        "    df_raw = pd.DataFrame([{\n",
        "        \"IMF\": \"Raw\",\n",
        "        \"Max\": np.max(serie_original),\n",
        "        \"Min\": np.min(serie_original),\n",
        "        \"Mean\": np.mean(serie_original),\n",
        "        \"Std\": np.std(serie_original),\n",
        "        \"PE\": None\n",
        "    }])\n",
        "\n",
        "    # Estad√≠sticas del residuo (incluye entrop√≠a)\n",
        "    df_residual = pd.DataFrame([{\n",
        "        \"IMF\": \"Residual\",\n",
        "        \"Max\": np.max(residual),\n",
        "        \"Min\": np.min(residual),\n",
        "        \"Mean\": np.mean(residual),\n",
        "        \"Std\": np.std(residual),\n",
        "        \"PE\": perm_entropy(residual, normalize=True)\n",
        "    }])\n",
        "\n",
        "    # Combina todas las estad√≠sticas en un √∫nico DataFrame\n",
        "    df_stats = pd.concat([df_raw, pd.DataFrame(df_stats), df_residual], ignore_index=True)\n",
        "\n",
        "    # Clasificaci√≥n de IMFs por frecuencia seg√∫n la mediana de entrop√≠a\n",
        "    pe_values = df_stats[df_stats[\"IMF\"].str.contains(\"IMF\")][\"PE\"]\n",
        "    threshold = pe_values.median()\n",
        "    df_stats[\"Freq\"] = df_stats[\"PE\"].apply(\n",
        "        lambda x: \"High\" if pd.notnull(x) and x >= threshold else (\"Low\" if pd.notnull(x) else None)\n",
        "    )\n",
        "\n",
        "    # Guardado en CSV si se especifica una estaci√≥n\n",
        "    if estacion:\n",
        "        carpeta = DIRS[\"stats_train\"] if modo == \"train\" else DIRS[\"stats_test\"]\n",
        "        archivo = f\"{estacion.replace('/', '_')}.csv\"\n",
        "        df_stats.to_csv(os.path.join(carpeta, archivo), index=False)\n",
        "        print(f\"üìÑ Estad√≠sticas guardadas: {archivo}\")\n",
        "\n",
        "    return df_stats\n",
        "\n",
        "# ====================================================\n",
        "# C√°lculo de estad√≠sticas para todas las estaciones\n",
        "# ====================================================\n",
        "\n",
        "stats_dict_train = {}\n",
        "stats_dict_test = {}\n",
        "\n",
        "for est, info in imfs_dict.items():\n",
        "    stats_dict_train[est] = calcular_estadisticas(\n",
        "        info['IMFs_train'], info['serie_train'], estacion=est, modo='train'\n",
        "    )\n",
        "    stats_dict_test[est] = calcular_estadisticas(\n",
        "        info['IMFs_test'], info['serie_test'], estacion=est, modo='test'\n",
        "    )\n"
      ],
      "metadata": {
        "id": "pY0bFvyAlnzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß™ Preparaci√≥n de datos para LSTM\n",
        "\n",
        "Se define la funci√≥n `preparar_serie_lstm(...)`, encargada de transformar una serie temporal en un conjunto de muestras de entrenamiento para una red LSTM.\n",
        "\n",
        "Esta transformaci√≥n se realiza utilizando una ventana deslizante:\n",
        "\n",
        "- Entrada `X`: subseries de longitud `n_steps`.\n",
        "- Salida `y`: el siguiente valor en la serie despu√©s de cada subserie.\n",
        "\n",
        "Esto permite que la red LSTM aprenda patrones secuenciales y realice predicciones basadas en contextos anteriores.\n"
      ],
      "metadata": {
        "id": "nNS7F6AVl9GH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# Preparaci√≥n de datos para entrenamiento de LSTM\n",
        "# =====================================================\n",
        "\n",
        "def preparar_serie_lstm(serie, n_steps=10):\n",
        "    \"\"\"\n",
        "    Genera muestras (X, y) para redes LSTM a partir de una serie temporal.\n",
        "\n",
        "    Par√°metros:\n",
        "        serie (np.ndarray): Serie normalizada con forma (n, 1) o (n,).\n",
        "        n_steps (int): N√∫mero de pasos de entrada (ventana temporal).\n",
        "\n",
        "    Retorna:\n",
        "        Tuple[np.ndarray, np.ndarray]: Arrays con muestras X y etiquetas y.\n",
        "    \"\"\"\n",
        "    X, y = [], []\n",
        "\n",
        "    # Crea secuencias deslizantes de longitud n_steps\n",
        "    for i in range(len(serie) - n_steps):\n",
        "        X.append(serie[i:i + n_steps])  # Subserie de entrada\n",
        "        y.append(serie[i + n_steps])    # Valor a predecir\n",
        "\n",
        "    return np.array(X), np.array(y)\n"
      ],
      "metadata": {
        "id": "mhemJZqPl9vN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ü§ñ Entrenamiento de modelos ARIMA y LSTM por frecuencia\n",
        "\n",
        "Esta celda define la funci√≥n `entrenar_modelos(...)`, que implementa el enfoque h√≠brido:\n",
        "\n",
        "- Para cada estaci√≥n:\n",
        "  - Clasifica los IMFs en **alta** o **baja frecuencia** (seg√∫n entrop√≠a).\n",
        "  - **IMFs de baja frecuencia**: se modelan con **ARIMA** (orden fijo (1,0,0)).\n",
        "  - **IMFs de alta frecuencia**: se modelan con **LSTM** usando una arquitectura secuencial.\n",
        "  - **Componente residual** (serie original menos suma de IMFs): se puede modelar opcionalmente con ARIMA.\n",
        "- Se guarda:\n",
        "  - Cada modelo ARIMA (por IMF y residual) como `.pkl`.\n",
        "  - Cada modelo LSTM entrenado como `.keras`.\n",
        "  - El historial de entrenamiento de LSTM en el diccionario `histories`.\n",
        "\n",
        "Se retorna:\n",
        "- `pred_arima`: predicci√≥n acumulada de los componentes modelados con ARIMA.\n",
        "- `pred_lstm`: predicci√≥n acumulada de los componentes modelados con LSTM.\n",
        "- `histories`: m√©tricas de p√©rdida por √©poca para cada LSTM.\n"
      ],
      "metadata": {
        "id": "Mwv8k5KnmRLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================================\n",
        "# Entrenamiento de modelos h√≠bridos ARIMA y LSTM por IMF\n",
        "# ========================================================\n",
        "\n",
        "def entrenar_modelos(imfs_dict, stats_dict, n_steps=10, usar_residual=True):\n",
        "    \"\"\"\n",
        "    Entrena modelos ARIMA (frecuencia baja y residual) y LSTM (frecuencia alta).\n",
        "\n",
        "    Par√°metros:\n",
        "        imfs_dict (dict): Diccionario con IMFs y series por estaci√≥n.\n",
        "        stats_dict (dict): Tabla con clasificaci√≥n de frecuencia por IMF.\n",
        "        n_steps (int): Tama√±o de la ventana para LSTM.\n",
        "        usar_residual (bool): Si se incluye modelo ARIMA sobre el residual.\n",
        "\n",
        "    Retorna:\n",
        "        Tuple[dict, dict, dict]: Predicciones ARIMA, LSTM y historiales LSTM.\n",
        "    \"\"\"\n",
        "    pred_arima, pred_lstm, histories = {}, {}, {}\n",
        "\n",
        "    for est, datos in imfs_dict.items():\n",
        "        print(f\"üîß Entrenando modelos para: {est}\")\n",
        "        nombre_archivo = est.replace('/', '_').replace(' ', '_')\n",
        "\n",
        "        imfs_train = datos['IMFs_train']\n",
        "        serie_train = datos['serie_train']\n",
        "        residual = serie_train - imfs_train.sum(axis=0)\n",
        "        df_stats = stats_dict[est]\n",
        "\n",
        "        # Inicializar predicciones\n",
        "        pred_arima[est] = np.zeros(len(serie_train))\n",
        "        pred_lstm[est] = np.zeros(len(serie_train))\n",
        "\n",
        "        # Clasifica los IMFs seg√∫n su frecuencia\n",
        "        altas, bajas = [], []\n",
        "        for _, row in df_stats.iterrows():\n",
        "            if isinstance(row[\"IMF\"], str) and \"IMF\" in row[\"IMF\"]:\n",
        "                idx = int(row[\"IMF\"].replace(\"IMF\", \"\")) - 1\n",
        "                (altas if row[\"Freq\"] == \"High\" else bajas).append(idx)\n",
        "\n",
        "        # ========================\n",
        "        # Modelado ARIMA (bajas)\n",
        "        # ========================\n",
        "        for i in bajas:\n",
        "            try:\n",
        "                modelo = ARIMA(imfs_train[i], order=(1, 0, 0)).fit()\n",
        "                pred = modelo.predict(start=1, end=len(imfs_train[i]) - 1)\n",
        "                pred = np.insert(pred, 0, 0)  # Alineaci√≥n con el inicio\n",
        "                pred_arima[est] += pred\n",
        "\n",
        "                # Guardar modelo ARIMA en disco\n",
        "                with open(f\"{DIRS['arima_models']}/{nombre_archivo}_imf{i}_arima.pkl\", \"wb\") as f:\n",
        "                    pickle.dump(modelo, f)\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è ARIMA IMF{i+1} - {e}\")\n",
        "\n",
        "        # ========================\n",
        "        # Modelado LSTM (altas)\n",
        "        # ========================\n",
        "        for i in altas:\n",
        "            serie = imfs_train[i].reshape(-1, 1)\n",
        "            scaler = MinMaxScaler()\n",
        "            serie_scaled = scaler.fit_transform(serie)\n",
        "\n",
        "            X, y = preparar_serie_lstm(serie_scaled, n_steps)\n",
        "            if len(X) == 0:\n",
        "                continue\n",
        "\n",
        "            X = X.reshape((X.shape[0], X.shape[1], 1))\n",
        "\n",
        "            # Definici√≥n del modelo LSTM\n",
        "            model = Sequential([\n",
        "                Input(shape=(n_steps, 1)),\n",
        "                LSTM(128),\n",
        "                Dropout(0.2),\n",
        "                Dense(1)\n",
        "            ])\n",
        "            model.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "            # Entrenamiento silencioso\n",
        "            hist = model.fit(X, y, epochs=100, verbose=0)\n",
        "            histories[f\"{est}_imf{i}\"] = hist.history\n",
        "\n",
        "            # Guardar modelo\n",
        "            model.save(f\"{DIRS['lstm_models']}/{nombre_archivo}_imf{i}_lstm.keras\")\n",
        "\n",
        "            # Predicciones sobre entrenamiento\n",
        "            y_pred = model.predict(X, verbose=0)\n",
        "            y_pred_inv = scaler.inverse_transform(y_pred).flatten()\n",
        "\n",
        "            pred_temp = np.zeros(len(serie_train))\n",
        "            pred_temp[n_steps:] = y_pred_inv  # Ajuste por ventana inicial\n",
        "            pred_lstm[est] += pred_temp\n",
        "\n",
        "        # ========================\n",
        "        # ARIMA para residual\n",
        "        # ========================\n",
        "        if usar_residual:\n",
        "            try:\n",
        "                modelo = ARIMA(residual, order=(1, 0, 0)).fit()\n",
        "                pred = modelo.predict(start=1, end=len(residual) - 1)\n",
        "                pred = np.insert(pred, 0, 0)\n",
        "                pred_arima[est] += pred\n",
        "\n",
        "                with open(f\"{DIRS['arima_models']}/{nombre_archivo}_residual_arima.pkl\", \"wb\") as f:\n",
        "                    pickle.dump(modelo, f)\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è Residual ARIMA - {e}\")\n",
        "\n",
        "    return pred_arima, pred_lstm, histories\n",
        "\n",
        "# ====================================\n",
        "# Ejecutar entrenamiento de los modelos\n",
        "# ====================================\n",
        "\n",
        "pred_arima, pred_lstm, histories = entrenar_modelos(\n",
        "    imfs_dict, stats_dict_train, n_steps=10\n",
        ")\n"
      ],
      "metadata": {
        "id": "iTq8ae2SmMM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß© Gr√°ficos de reconstrucci√≥n del conjunto de entrenamiento\n",
        "\n",
        "Esta celda define la funci√≥n `graficar_reconstruccion(...)`, que:\n",
        "\n",
        "- Toma las predicciones de ARIMA y LSTM para cada estaci√≥n.\n",
        "- Suma ambas para generar la **serie reconstruida** del entrenamiento.\n",
        "- Compara la serie real con la reconstruida mediante un gr√°fico temporal.\n",
        "- Guarda cada gr√°fico en la ruta `graficos_train_reconstruccion`.\n",
        "\n",
        "Esta visualizaci√≥n permite evaluar visualmente qu√© tan bien el modelo h√≠brido reproduce la serie original.\n"
      ],
      "metadata": {
        "id": "-BYwdFOjmncg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ========================================================\n",
        "# Gr√°fico de reconstrucci√≥n: Real vs. Predicci√≥n Total\n",
        "# ========================================================\n",
        "\n",
        "def graficar_reconstruccion(imfs_dict, pred_arima, pred_lstm):\n",
        "    for est in imfs_dict:\n",
        "        serie_real = imfs_dict[est]['serie_train']\n",
        "        fechas = imfs_dict[est]['fechas'][:len(serie_real)]\n",
        "\n",
        "        # Verifica que existan predicciones para la estaci√≥n\n",
        "        if est not in pred_arima or est not in pred_lstm:\n",
        "            continue\n",
        "\n",
        "        # Suma de predicciones ARIMA + LSTM\n",
        "        pred_total = pred_arima[est] + pred_lstm[est]\n",
        "\n",
        "        # Genera el gr√°fico\n",
        "        plt.figure(figsize=(14, 5))\n",
        "        plt.plot(fechas, serie_real, label=\"Serie Real\", color='black')\n",
        "        plt.plot(fechas, pred_total, '--', label=\"Reconstrucci√≥n\", color='green')\n",
        "        plt.title(f\"Reconstrucci√≥n del conjunto de entrenamiento - {est}\")\n",
        "        plt.xlabel(\"Fecha\")\n",
        "        plt.ylabel(\"Precipitaci√≥n (mm)\")\n",
        "        plt.legend()\n",
        "        plt.grid(True, linestyle='--', alpha=0.5)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Guardar gr√°fico en la carpeta correspondiente\n",
        "        ruta = os.path.join(\n",
        "            DIRS[\"graficos_train_reconstruccion\"],\n",
        "            f\"{est.replace('/', '_').replace(' ', '_')}_reconstruccion_train.png\"\n",
        "        )\n",
        "        plt.savefig(ruta, dpi=300)\n",
        "        plt.show()\n",
        "        plt.close()\n"
      ],
      "metadata": {
        "id": "sQLcbn3_mnve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìà Evaluaci√≥n cuantitativa del conjunto de entrenamiento\n",
        "\n",
        "Se define la funci√≥n `evaluar_modelos(...)`, que eval√∫a el desempe√±o de los modelos entrenados para cada estaci√≥n usando las siguientes m√©tricas:\n",
        "\n",
        "- **RMSE**: ra√≠z del error cuadr√°tico medio.\n",
        "- **MAE**: error absoluto medio.\n",
        "- **MSE**: error cuadr√°tico medio.\n",
        "- **R¬≤**: coeficiente de determinaci√≥n.\n",
        "\n",
        "Se calcula el rendimiento por separado para:\n",
        "- Predicci√≥n parcial por ARIMA.\n",
        "- Predicci√≥n parcial por LSTM.\n",
        "- Predicci√≥n total (ARIMA + LSTM).\n",
        "\n",
        "Los resultados se imprimen por estaci√≥n y ayudan a comparar la contribuci√≥n relativa de cada modelo.\n"
      ],
      "metadata": {
        "id": "v-aFE0PSmztp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Evaluaci√≥n num√©rica del desempe√±o de los modelos (entrenamiento)\n",
        "# ============================================================\n",
        "\n",
        "def evaluar_modelos(imfs_dict, pred_arima, pred_lstm):\n",
        "    print(\"üìä Evaluaci√≥n en conjunto de entrenamiento\")\n",
        "\n",
        "    for est in imfs_dict:\n",
        "        # Serie real observada\n",
        "        real = imfs_dict[est]['serie_train']\n",
        "\n",
        "        # Predicciones parciales\n",
        "        pred_a = pred_arima.get(est)\n",
        "        pred_l = pred_lstm.get(est)\n",
        "\n",
        "        if pred_a is None or pred_l is None:\n",
        "            continue\n",
        "\n",
        "        # Predicci√≥n total (modelo h√≠brido)\n",
        "        pred_total = pred_a + pred_l\n",
        "\n",
        "        print(f\"\\nüìç Estaci√≥n: {est}\")\n",
        "        for nombre, pred in [(\"ARIMA\", pred_a), (\"LSTM\", pred_l), (\"Total\", pred_total)]:\n",
        "            rmse = np.sqrt(mean_squared_error(real, pred))\n",
        "            mae = mean_absolute_error(real, pred)\n",
        "            mse = mean_squared_error(real, pred)\n",
        "            r2 = r2_score(real, pred)\n",
        "            print(f\"  {nombre:6s} ‚Üí RMSE: {rmse:.2f}, MAE: {mae:.2f}, MSE: {mse:.2f}, R¬≤: {r2:.2f}\")\n"
      ],
      "metadata": {
        "id": "xKztSvOXm0BD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìâ Visualizaci√≥n de curvas de aprendizaje (MSE por √©poca)\n",
        "\n",
        "Esta celda define la funci√≥n `graficar_curvas_aprendizaje_mse(...)`, que:\n",
        "\n",
        "- Recorre el historial de entrenamiento de los modelos LSTM para cada IMF de cada estaci√≥n.\n",
        "- Grafica la evoluci√≥n del error cuadr√°tico medio (MSE) a lo largo de las √©pocas.\n",
        "- Guarda una imagen por estaci√≥n en la ruta:  \n",
        "  `graficas/resultados_modelo/train/curvas_mse/`.\n",
        "\n",
        "Estas curvas permiten detectar:\n",
        "- Convergencia adecuada del modelo.\n",
        "- Posible sobreajuste (overfitting).\n",
        "- Desempe√±o por componente IMF.\n"
      ],
      "metadata": {
        "id": "jfacnCdhnMqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================\n",
        "# Gr√°ficas de curvas de aprendizaje de LSTM (MSE por √©pocas)\n",
        "# ===========================================================\n",
        "\n",
        "def graficar_curvas_aprendizaje_mse(histories, base_dir=BASE_DIR):\n",
        "    # Directorio de salida para las curvas\n",
        "    output_dir = os.path.join(base_dir, \"graficas\", \"resultados_modelo\", \"train\", \"curvas_mse\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Estaciones con historiales disponibles (ignorando residuales)\n",
        "    estaciones = sorted(set(k.split('_')[0] for k in histories if \"residual\" not in k))\n",
        "\n",
        "    for estacion in estaciones:\n",
        "        # Obtiene claves IMF asociadas a la estaci√≥n\n",
        "        claves = [k for k in histories if k.startswith(estacion) and \"residual\" not in k]\n",
        "        if not claves:\n",
        "            print(f\"‚ö†Ô∏è No hay historiales para estaci√≥n: {estacion}\")\n",
        "            continue\n",
        "\n",
        "        # Plot de la curva de entrenamiento por IMF\n",
        "        plt.figure(figsize=(10, 5))\n",
        "        for clave in claves:\n",
        "            historial = histories[clave]\n",
        "            componente = clave.split('_')[-1]  # Ej: imf2\n",
        "            plt.plot(historial['loss'], label=f\"{componente}\")\n",
        "\n",
        "        plt.title(f\"Curva de aprendizaje - {estacion}\")\n",
        "        plt.xlabel(\"√âpocas\")\n",
        "        plt.ylabel(\"MSE\")\n",
        "        plt.grid(True, linestyle='--', alpha=0.5)\n",
        "        plt.legend(title=\"IMF\")\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Guardar gr√°fico\n",
        "        nombre = estacion.replace(\"/\", \"_\").replace(\" \", \"_\") + \"_mse_epochs_train.png\"\n",
        "        plt.savefig(os.path.join(output_dir, nombre), dpi=300)\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "    print(f\"‚úÖ Curvas de aprendizaje guardadas en: {output_dir}\")\n"
      ],
      "metadata": {
        "id": "2TdCg54rm9xv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìå Dispersi√≥n: Valor real vs. predicho (conjunto de entrenamiento)\n",
        "\n",
        "Esta celda define la funci√≥n `graficar_dispersi√≥n_entrenamiento(...)`, que:\n",
        "\n",
        "- Calcula la **reconstrucci√≥n total** (ARIMA + LSTM) para cada estaci√≥n.\n",
        "- Genera un gr√°fico de dispersi√≥n que compara los valores reales y los predichos.\n",
        "- Dibuja la l√≠nea de identidad (roja discontinua), que representa una predicci√≥n perfecta.\n",
        "- Guarda cada gr√°fico en la carpeta:  \n",
        "  `graficas/resultados_modelo/train/dispersion`.\n",
        "\n",
        "Esta visualizaci√≥n ayuda a:\n",
        "- Evaluar el **alineamiento entre la predicci√≥n y los datos reales**.\n",
        "- Identificar **sesgos sistem√°ticos o errores de predicci√≥n**.\n"
      ],
      "metadata": {
        "id": "MQ8JgYjsnUGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Gr√°fica de dispersi√≥n Real vs Predicci√≥n (entrenamiento)\n",
        "# ============================================================\n",
        "\n",
        "def graficar_dispersi√≥n_entrenamiento(imfs_dict, pred_arima, pred_lstm, base_dir=BASE_DIR):\n",
        "    # Reconstrucci√≥n h√≠brida (ARIMA + LSTM)\n",
        "    reconstruccion_total_train = {\n",
        "        est: pred_arima[est] + pred_lstm[est]\n",
        "        for est in pred_arima\n",
        "    }\n",
        "\n",
        "    # Directorio de salida\n",
        "    output_dir = os.path.join(base_dir, \"graficas\", \"resultados_modelo\", \"train\", \"dispersion\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    for estacion, predicha in reconstruccion_total_train.items():\n",
        "        real = imfs_dict[estacion]['serie_train']\n",
        "\n",
        "        plt.figure(figsize=(7, 7))\n",
        "        sns.scatterplot(\n",
        "            x=real, y=predicha,\n",
        "            alpha=0.6, color='dodgerblue',\n",
        "            edgecolor='k', s=60\n",
        "        )\n",
        "\n",
        "        # L√≠nea de identidad (predicci√≥n perfecta)\n",
        "        min_val = min(np.min(real), np.min(predicha))\n",
        "        max_val = max(np.max(real), np.max(predicha))\n",
        "        plt.plot([min_val, max_val], [min_val, max_val], '--', color='red', label='L√≠nea perfecta')\n",
        "\n",
        "        plt.xlabel(\"Valor real\")\n",
        "        plt.ylabel(\"Valor predicho\")\n",
        "        plt.title(f\"{estacion} ‚Äî Dispersi√≥n Real vs Predicci√≥n\")\n",
        "        plt.grid(True, linestyle='--', alpha=0.5)\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Guardar figura\n",
        "        nombre_archivo = estacion.replace(\"/\", \"_\").replace(\" \", \"_\") + \"_dispersion.png\"\n",
        "        ruta_salida = os.path.join(output_dir, nombre_archivo)\n",
        "        plt.savefig(ruta_salida, dpi=300)\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "    print(f\"‚úÖ Gr√°ficas de dispersi√≥n guardadas en: {output_dir}\")\n"
      ],
      "metadata": {
        "id": "_V-3c14BnUS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß™ Evaluaci√≥n del conjunto de entrenamiento\n",
        "\n",
        "En esta celda se ejecutan las funciones de evaluaci√≥n y visualizaci√≥n para el conjunto de entrenamiento. Se incluyen:\n",
        "\n",
        "1. **Reconstrucci√≥n temporal**: comparaci√≥n entre la serie real y la reconstruida (ARIMA + LSTM).\n",
        "2. **Evaluaci√≥n cuantitativa**: c√°lculo de m√©tricas de error (RMSE, MAE, MSE, R¬≤) por componente y total.\n",
        "3. **Curvas de aprendizaje**: evoluci√≥n del error MSE durante el entrenamiento de cada LSTM por IMF.\n",
        "4. **Gr√°ficos de dispersi√≥n**: comparaci√≥n punto a punto entre valores reales y predichos para cada estaci√≥n.\n",
        "\n",
        "Estas visualizaciones y m√©tricas son esenciales para validar la calidad del modelo h√≠brido en el conjunto de entrenamiento.\n"
      ],
      "metadata": {
        "id": "kTSHJ2cJnsjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================\n",
        "# Evaluaci√≥n del desempe√±o en entrenamiento\n",
        "# =============================================\n",
        "\n",
        "# 1. Reconstrucci√≥n visual por estaci√≥n\n",
        "graficar_reconstruccion(imfs_dict, pred_arima, pred_lstm)\n",
        "\n",
        "# 2. M√©tricas cuantitativas (RMSE, MAE, MSE, R¬≤)\n",
        "evaluar_modelos(imfs_dict, pred_arima, pred_lstm)\n",
        "\n",
        "# 3. Curvas de aprendizaje (MSE vs. √©pocas)\n",
        "graficar_curvas_aprendizaje_mse(histories)\n",
        "\n",
        "# 4. Dispersi√≥n Real vs Predicci√≥n\n",
        "graficar_dispersi√≥n_entrenamiento(imfs_dict, pred_arima, pred_lstm)\n"
      ],
      "metadata": {
        "id": "UAH8dVmhni5C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß™ Evaluaci√≥n del modelo h√≠brido en el conjunto de prueba\n",
        "\n",
        "Esta celda define la funci√≥n `evaluar_modelos_test(...)`, que:\n",
        "\n",
        "- Aplica los modelos ya entrenados (ARIMA y LSTM) a los datos de prueba (`IMFs_test` y `residual_test`) de cada estaci√≥n.\n",
        "- Para cada estaci√≥n:\n",
        "  - Carga los modelos ARIMA desde disco (`.pkl`).\n",
        "  - Carga los modelos LSTM desde disco (`.keras`) y genera predicciones a partir de las subseries del test.\n",
        "  - Calcula la predicci√≥n total como la suma de ARIMA + LSTM.\n",
        "- Eval√∫a el desempe√±o utilizando las m√©tricas:\n",
        "  - **RMSE**: ra√≠z del error cuadr√°tico medio.\n",
        "  - **MAE**: error absoluto medio.\n",
        "  - **MSE**: error cuadr√°tico medio.\n",
        "  - **R¬≤**: coeficiente de determinaci√≥n.\n",
        "\n",
        "Este procedimiento permite validar la capacidad de generalizaci√≥n del modelo h√≠brido sobre datos no vistos.\n"
      ],
      "metadata": {
        "id": "Nv-Kn0OknyQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Evaluaci√≥n del modelo h√≠brido en el conjunto de prueba\n",
        "# ============================================================\n",
        "\n",
        "def evaluar_modelos_test(imfs_dict, stats_dict_test, n_steps=10):\n",
        "    \"\"\"\n",
        "    Eval√∫a el modelo h√≠brido en el conjunto de prueba.\n",
        "\n",
        "    Par√°metros:\n",
        "        imfs_dict (dict): Diccionario con datos de prueba por estaci√≥n.\n",
        "        stats_dict_test (dict): Clasificaci√≥n de IMFs para el test.\n",
        "        n_steps (int): Longitud de ventana para LSTM.\n",
        "    \"\"\"\n",
        "    print(\"üìä Evaluaci√≥n en conjunto de prueba\")\n",
        "\n",
        "    for est, datos in imfs_dict.items():\n",
        "        print(f\"\\nüìç Estaci√≥n: {est}\")\n",
        "        nombre_archivo = est.replace('/', '_').replace(' ', '_')\n",
        "\n",
        "        imfs_test = datos['IMFs_test']\n",
        "        serie_test = datos['serie_test']\n",
        "        residual_test = serie_test - imfs_test.sum(axis=0)\n",
        "        df_stats = stats_dict_test[est]\n",
        "\n",
        "        # Inicializar predicciones vac√≠as\n",
        "        pred_arima = np.zeros(len(serie_test))\n",
        "        pred_lstm = np.zeros(len(serie_test))\n",
        "\n",
        "        # Clasifica los IMFs seg√∫n frecuencia\n",
        "        altas, bajas = [], []\n",
        "        for _, row in df_stats.iterrows():\n",
        "            if \"IMF\" in str(row[\"IMF\"]):\n",
        "                idx = int(row[\"IMF\"].replace(\"IMF\", \"\")) - 1\n",
        "                (altas if row[\"Freq\"] == \"High\" else bajas).append(idx)\n",
        "\n",
        "        # ========================\n",
        "        # ARIMA - IMFs de baja frecuencia\n",
        "        # ========================\n",
        "        for i in bajas:\n",
        "            try:\n",
        "                model_path = os.path.join(DIRS[\"arima_models\"], f\"{nombre_archivo}_imf{i}_arima.pkl\")\n",
        "                with open(model_path, \"rb\") as f:\n",
        "                    modelo = pickle.load(f)\n",
        "                pred = modelo.predict(start=len(modelo.model.endog), end=len(modelo.model.endog) + len(imfs_test[i]) - 1)\n",
        "                pred_arima += pred\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è ARIMA IMF{i+1} - {e}\")\n",
        "\n",
        "        # ========================\n",
        "        # LSTM - IMFs de alta frecuencia\n",
        "        # ========================\n",
        "        for i in altas:\n",
        "            try:\n",
        "                serie = imfs_test[i].reshape(-1, 1)\n",
        "                scaler = MinMaxScaler()\n",
        "                serie_scaled = scaler.fit_transform(serie)\n",
        "                X_test, _ = preparar_serie_lstm(serie_scaled, n_steps)\n",
        "                if len(X_test) == 0:\n",
        "                    continue\n",
        "                X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "                model_path = os.path.join(DIRS[\"lstm_models\"], f\"{nombre_archivo}_imf{i}_lstm.keras\")\n",
        "                model = load_model(model_path)\n",
        "                y_pred = model.predict(X_test, verbose=0)\n",
        "                y_pred_inv = scaler.inverse_transform(y_pred).flatten()\n",
        "\n",
        "                pred_temp = np.zeros(len(serie_test))\n",
        "                pred_temp[n_steps:] = y_pred_inv\n",
        "                pred_lstm += pred_temp\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è LSTM IMF{i+1} - {e}\")\n",
        "\n",
        "        # ========================\n",
        "        # ARIMA - Residual\n",
        "        # ========================\n",
        "        try:\n",
        "            model_path = os.path.join(DIRS[\"arima_models\"], f\"{nombre_archivo}_residual_arima.pkl\")\n",
        "            with open(model_path, \"rb\") as f:\n",
        "                modelo = pickle.load(f)\n",
        "            pred = modelo.predict(start=len(modelo.model.endog), end=len(modelo.model.endog) + len(residual_test) - 1)\n",
        "            pred_arima += pred\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Residual ARIMA - {e}\")\n",
        "\n",
        "        # ========================\n",
        "        # Evaluaci√≥n final\n",
        "        # ========================\n",
        "        pred_total = pred_arima + pred_lstm\n",
        "\n",
        "        for nombre, pred in [(\"ARIMA\", pred_arima), (\"LSTM\", pred_lstm), (\"Total\", pred_total)]:\n",
        "            rmse = np.sqrt(mean_squared_error(serie_test, pred))\n",
        "            mae = mean_absolute_error(serie_test, pred)\n",
        "            mse = mean_squared_error(serie_test, pred)\n",
        "            r2 = r2_score(serie_test, pred)\n",
        "            print(f\"  {nombre:6s} ‚Üí RMSE: {rmse:.2f}, MAE: {mae:.2f}, MSE: {mse:.2f}, R¬≤: {r2:.2f}\")\n"
      ],
      "metadata": {
        "id": "-GAPsleXnxBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üñºÔ∏è Reconstrucci√≥n del conjunto de prueba\n",
        "\n",
        "Se define la funci√≥n `graficar_reconstruccion_test(...)`, que:\n",
        "\n",
        "- Aplica los modelos entrenados (ARIMA y LSTM) a los datos de prueba (`IMFs_test`, `residual_test`).\n",
        "- Calcula la predicci√≥n total por estaci√≥n.\n",
        "- Grafica la serie real del conjunto de prueba vs. la serie reconstruida.\n",
        "- Guarda cada gr√°fico en la ruta:\n",
        "  `graficas/resultados_modelo/test/reconstruccion`.\n",
        "\n",
        "Estas gr√°ficas permiten validar visualmente la calidad de las predicciones sobre datos no vistos.\n"
      ],
      "metadata": {
        "id": "Vk10_QaKoBoF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# Reconstrucci√≥n visual del conjunto de prueba\n",
        "# ============================================================\n",
        "\n",
        "def graficar_reconstruccion_test(imfs_dict, stats_dict_test, n_steps=10):\n",
        "    print(\"üñºÔ∏è Generando gr√°ficas de reconstrucci√≥n (test)...\")\n",
        "\n",
        "    for est, datos in imfs_dict.items():\n",
        "        nombre_archivo = est.replace('/', '_').replace(' ', '_')\n",
        "\n",
        "        imfs_test = datos['IMFs_test']\n",
        "        serie_test = datos['serie_test']\n",
        "        fechas_test = datos['fechas'][-len(serie_test):]\n",
        "        df_stats = stats_dict_test[est]\n",
        "        residual_test = serie_test - imfs_test.sum(axis=0)\n",
        "\n",
        "        pred_arima = np.zeros(len(serie_test))\n",
        "        pred_lstm = np.zeros(len(serie_test))\n",
        "\n",
        "        # Clasificaci√≥n de IMFs por frecuencia\n",
        "        altas, bajas = [], []\n",
        "        for _, row in df_stats.iterrows():\n",
        "            if \"IMF\" in str(row[\"IMF\"]):\n",
        "                idx = int(row[\"IMF\"].replace(\"IMF\", \"\")) - 1\n",
        "                (altas if row[\"Freq\"] == \"High\" else bajas).append(idx)\n",
        "\n",
        "        # Predicci√≥n con modelos ARIMA (IMFs de baja frecuencia)\n",
        "        for i in bajas:\n",
        "            try:\n",
        "                with open(os.path.join(DIRS[\"arima_models\"], f\"{nombre_archivo}_imf{i}_arima.pkl\"), \"rb\") as f:\n",
        "                    modelo = pickle.load(f)\n",
        "                pred = modelo.predict(start=len(modelo.model.endog), end=len(modelo.model.endog) + len(imfs_test[i]) - 1)\n",
        "                pred_arima += pred\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        # Predicci√≥n con modelos LSTM (IMFs de alta frecuencia)\n",
        "        for i in altas:\n",
        "            try:\n",
        "                serie = imfs_test[i].reshape(-1, 1)\n",
        "                scaler = MinMaxScaler()\n",
        "                serie_scaled = scaler.fit_transform(serie)\n",
        "                X_test, _ = preparar_serie_lstm(serie_scaled, n_steps)\n",
        "                if len(X_test) == 0:\n",
        "                    continue\n",
        "                X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "                model = load_model(os.path.join(DIRS[\"lstm_models\"], f\"{nombre_archivo}_imf{i}_lstm.keras\"))\n",
        "                y_pred = model.predict(X_test, verbose=0)\n",
        "                y_pred_inv = scaler.inverse_transform(y_pred).flatten()\n",
        "\n",
        "                pred_temp = np.zeros(len(serie_test))\n",
        "                pred_temp[n_steps:] = y_pred_inv\n",
        "                pred_lstm += pred_temp\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        # Predicci√≥n con ARIMA para el residual\n",
        "        try:\n",
        "            with open(os.path.join(DIRS[\"arima_models\"], f\"{nombre_archivo}_residual_arima.pkl\"), \"rb\") as f:\n",
        "                modelo = pickle.load(f)\n",
        "            pred = modelo.predict(start=len(modelo.model.endog), end=len(modelo.model.endog) + len(residual_test) - 1)\n",
        "            pred_arima += pred\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # Reconstrucci√≥n total\n",
        "        pred_total = pred_arima + pred_lstm\n",
        "\n",
        "        # Gr√°fico de reconstrucci√≥n\n",
        "        plt.figure(figsize=(14, 5))\n",
        "        plt.plot(fechas_test, serie_test, label=\"Serie Real (Test)\", color='black')\n",
        "        plt.plot(fechas_test, pred_total, '--', label=\"Reconstrucci√≥n (Test)\", color='green')\n",
        "        plt.title(f\"Reconstrucci√≥n del conjunto de prueba - {est}\")\n",
        "        plt.xlabel(\"Fecha\")\n",
        "        plt.ylabel(\"Precipitaci√≥n (mm)\")\n",
        "        plt.legend()\n",
        "        plt.grid(True, linestyle='--', alpha=0.5)\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Guardado\n",
        "        ruta = os.path.join(\n",
        "            DIRS[\"graficos_test_reconstruccion\"],\n",
        "            f\"{nombre_archivo}_reconstruccion_test.png\"\n",
        "        )\n",
        "        os.makedirs(os.path.dirname(ruta), exist_ok=True)\n",
        "        plt.savefig(ruta, dpi=300)\n",
        "        plt.show()\n",
        "        plt.close()\n"
      ],
      "metadata": {
        "id": "QF73BIS8oB5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìå Dispersi√≥n: Valor real vs. predicho (conjunto de prueba)\n",
        "\n",
        "Esta celda define la funci√≥n `graficar_dispersi√≥n_test(...)`, que:\n",
        "\n",
        "- Suma las predicciones generadas por los modelos ARIMA y LSTM sobre los `IMFs_test` y el residual.\n",
        "- Compara gr√°ficamente los valores reales y predichos de precipitaci√≥n mensual.\n",
        "- Dibuja la **l√≠nea de identidad** (l√≠nea perfecta en rojo), que representa una predicci√≥n ideal.\n",
        "- Guarda cada gr√°fico en:\n",
        "  `graficas/resultados_modelo/test/dispersion`.\n",
        "\n",
        "Estas gr√°ficas permiten validar visualmente el ajuste del modelo h√≠brido sobre datos de prueba (no vistos durante el entrenamiento).\n"
      ],
      "metadata": {
        "id": "Cln4n_98oSRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# Gr√°fica de dispersi√≥n Real vs Predicci√≥n (conjunto de prueba)\n",
        "# =============================================================\n",
        "\n",
        "def graficar_dispersi√≥n_test(imfs_dict, stats_dict_test, n_steps=10):\n",
        "    print(\"üîç Generando gr√°ficas de dispersi√≥n (test)...\")\n",
        "\n",
        "    output_dir = os.path.join(BASE_DIR, \"graficas\", \"resultados_modelo\", \"test\", \"dispersion\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    for est, datos in imfs_dict.items():\n",
        "        nombre_archivo = est.replace('/', '_').replace(' ', '_')\n",
        "\n",
        "        imfs_test = datos['IMFs_test']\n",
        "        serie_test = datos['serie_test']\n",
        "        df_stats = stats_dict_test[est]\n",
        "        residual_test = serie_test - imfs_test.sum(axis=0)\n",
        "\n",
        "        pred_arima = np.zeros(len(serie_test))\n",
        "        pred_lstm = np.zeros(len(serie_test))\n",
        "\n",
        "        # Clasificaci√≥n de IMFs por entrop√≠a\n",
        "        altas, bajas = [], []\n",
        "        for _, row in df_stats.iterrows():\n",
        "            if \"IMF\" in str(row[\"IMF\"]):\n",
        "                idx = int(row[\"IMF\"].replace(\"IMF\", \"\")) - 1\n",
        "                (altas if row[\"Freq\"] == \"High\" else bajas).append(idx)\n",
        "\n",
        "        # Predicci√≥n ARIMA para IMFs de baja frecuencia\n",
        "        for i in bajas:\n",
        "            try:\n",
        "                with open(os.path.join(DIRS[\"arima_models\"], f\"{nombre_archivo}_imf{i}_arima.pkl\"), \"rb\") as f:\n",
        "                    modelo = pickle.load(f)\n",
        "                pred_arima += modelo.predict(start=len(modelo.model.endog), end=len(modelo.model.endog) + len(imfs_test[i]) - 1)\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        # Predicci√≥n LSTM para IMFs de alta frecuencia\n",
        "        for i in altas:\n",
        "            try:\n",
        "                serie = imfs_test[i].reshape(-1, 1)\n",
        "                scaler = MinMaxScaler()\n",
        "                serie_scaled = scaler.fit_transform(serie)\n",
        "                X_test, _ = preparar_serie_lstm(serie_scaled, n_steps)\n",
        "                if len(X_test) == 0:\n",
        "                    continue\n",
        "                X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "                model = load_model(os.path.join(DIRS[\"lstm_models\"], f\"{nombre_archivo}_imf{i}_lstm.keras\"))\n",
        "                y_pred = model.predict(X_test, verbose=0)\n",
        "                y_pred_inv = scaler.inverse_transform(y_pred).flatten()\n",
        "                pred_temp = np.zeros(len(serie_test))\n",
        "                pred_temp[n_steps:] = y_pred_inv\n",
        "                pred_lstm += pred_temp\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "        # Predicci√≥n ARIMA para el residual\n",
        "        try:\n",
        "            with open(os.path.join(DIRS[\"arima_models\"], f\"{nombre_archivo}_residual_arima.pkl\"), \"rb\") as f:\n",
        "                modelo = pickle.load(f)\n",
        "            pred_arima += modelo.predict(start=len(modelo.model.endog), end=len(modelo.model.endog) + len(residual_test) - 1)\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "        # Suma total\n",
        "        pred_total = pred_arima + pred_lstm\n",
        "\n",
        "        # Gr√°fico de dispersi√≥n\n",
        "        plt.figure(figsize=(7, 7))\n",
        "        sns.scatterplot(x=serie_test, y=pred_total, alpha=0.6, color='dodgerblue', edgecolor='k', s=60)\n",
        "\n",
        "        min_val = min(np.min(serie_test), np.min(pred_total))\n",
        "        max_val = max(np.max(serie_test), np.max(pred_total))\n",
        "        plt.plot([min_val, max_val], [min_val, max_val], '--', color='red', label='L√≠nea perfecta')\n",
        "\n",
        "        plt.xlabel(\"Valor real (test)\")\n",
        "        plt.ylabel(\"Valor predicho (test)\")\n",
        "        plt.title(f\"{est} ‚Äî Dispersi√≥n Real vs Predicci√≥n (test)\")\n",
        "        plt.grid(True, linestyle='--', alpha=0.5)\n",
        "        plt.legend()\n",
        "        plt.tight_layout()\n",
        "\n",
        "        plt.savefig(os.path.join(output_dir, f\"{nombre_archivo}_dispersion_test.png\"), dpi=300)\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "    print(f\"‚úÖ Gr√°ficas de dispersi√≥n guardadas en: {output_dir}\")\n"
      ],
      "metadata": {
        "id": "-RxS36wqoSgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üß™ Evaluaci√≥n del conjunto de prueba (Test)\n",
        "\n",
        "En esta celda se ejecutan las funciones de evaluaci√≥n y visualizaci√≥n del modelo h√≠brido aplicadas al conjunto de prueba. Se incluyen:\n",
        "\n",
        "1. **Evaluaci√≥n cuantitativa**: C√°lculo de m√©tricas de error (RMSE, MAE, MSE, R¬≤) para ARIMA, LSTM y la suma de ambos.\n",
        "2. **Reconstrucci√≥n temporal**: Comparaci√≥n entre la serie real de prueba y la reconstrucci√≥n del modelo.\n",
        "3. **Dispersi√≥n Real vs Predicci√≥n**: Visualizaci√≥n punto a punto que permite evaluar el alineamiento entre la predicci√≥n y los valores observados.\n",
        "\n",
        "Estas herramientas permiten validar la capacidad de generalizaci√≥n del modelo sobre datos no utilizados durante el entrenamiento.\n"
      ],
      "metadata": {
        "id": "D2KEAuj9ogGl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ======================================================\n",
        "# Evaluaci√≥n y visualizaci√≥n del conjunto de prueba\n",
        "# ======================================================\n",
        "\n",
        "# 1. Evaluaci√≥n num√©rica (m√©tricas por estaci√≥n)\n",
        "evaluar_modelos_test(imfs_dict, stats_dict_test, n_steps=10)\n",
        "\n",
        "# 2. Reconstrucci√≥n gr√°fica de la serie (test)\n",
        "graficar_reconstruccion_test(imfs_dict, stats_dict_test, n_steps=10)\n",
        "\n",
        "# 3. Gr√°fica de dispersi√≥n Real vs Predicci√≥n (test)\n",
        "graficar_dispersi√≥n_test(imfs_dict, stats_dict_test, n_steps=10)\n"
      ],
      "metadata": {
        "id": "KoCjPgL6ogdM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìÖ Predicci√≥n futura (proyecci√≥n mensual)\n",
        "\n",
        "Esta celda define la funci√≥n `generar_predicciones_futuras(...)`, que:\n",
        "\n",
        "- Utiliza los modelos ya entrenados (ARIMA y LSTM) para proyectar la precipitaci√≥n mensual en cada estaci√≥n durante los pr√≥ximos `pasos` meses (por defecto: 35).\n",
        "- Aplica:\n",
        "  - Modelos ARIMA para los IMFs de baja frecuencia.\n",
        "  - Modelos LSTM con predicci√≥n autoregresiva para los IMFs de alta frecuencia.\n",
        "  - ARIMA para el componente residual.\n",
        "- Suma todas las predicciones por componente para obtener la serie proyectada total.\n",
        "- Genera una tabla por estaci√≥n con las fechas futuras y sus valores predichos.\n",
        "- Retorna un diccionario con DataFrames estructurados como:  \n",
        "  `{estaci√≥n: DataFrame con columnas ['Fecha', 'Prediccion']}`.\n",
        "\n",
        "Este procedimiento permite extender las estimaciones del modelo m√°s all√° del periodo observado.\n"
      ],
      "metadata": {
        "id": "yQ7-I9FZoyrv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================================================\n",
        "# Generaci√≥n de predicciones futuras (35 meses por estaci√≥n)\n",
        "# ===========================================================\n",
        "\n",
        "def generar_predicciones_futuras(imfs_dict, stats_dict_train, n_steps=10, pasos=35):\n",
        "    \"\"\"\n",
        "    Genera predicciones futuras de precipitaci√≥n para cada estaci√≥n.\n",
        "\n",
        "    Par√°metros:\n",
        "        imfs_dict (dict): Contiene IMFs, serie completa y fechas.\n",
        "        stats_dict_train (dict): Estad√≠sticas y clasificaci√≥n de IMFs.\n",
        "        n_steps (int): Tama√±o de la ventana para LSTM.\n",
        "        pasos (int): Meses a predecir.\n",
        "\n",
        "    Retorna:\n",
        "        dict: {estaci√≥n: DataFrame con columnas ['Fecha', 'Prediccion']}\n",
        "    \"\"\"\n",
        "    predicciones_futuras = {}\n",
        "\n",
        "    for est, datos in imfs_dict.items():\n",
        "        print(f\"üìà Prediciendo futuro para estaci√≥n: {est}\")\n",
        "        nombre_archivo = est.replace('/', '_').replace(' ', '_')\n",
        "\n",
        "        imfs_full = datos['IMFs']\n",
        "        serie_full = datos['serie']\n",
        "        fechas_full = datos['fechas']\n",
        "        df_stats = stats_dict_train[est]\n",
        "\n",
        "        pred_total = np.zeros(pasos)\n",
        "        altas, bajas = [], []\n",
        "\n",
        "        # Clasificaci√≥n por entrop√≠a\n",
        "        for _, row in df_stats.iterrows():\n",
        "            if \"IMF\" in str(row[\"IMF\"]):\n",
        "                idx = int(row[\"IMF\"].replace(\"IMF\", \"\")) - 1\n",
        "                (altas if row[\"Freq\"] == \"High\" else bajas).append(idx)\n",
        "\n",
        "        # ========================\n",
        "        # Predicci√≥n con ARIMA\n",
        "        # ========================\n",
        "        for i in bajas:\n",
        "            try:\n",
        "                model_path = os.path.join(DIRS[\"arima_models\"], f\"{nombre_archivo}_imf{i}_arima.pkl\")\n",
        "                with open(model_path, \"rb\") as f:\n",
        "                    modelo = pickle.load(f)\n",
        "                pred = modelo.predict(start=len(modelo.model.endog), end=len(modelo.model.endog) + pasos - 1)\n",
        "                pred_total += pred\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è ARIMA IMF{i+1} - {e}\")\n",
        "\n",
        "        # ========================\n",
        "        # Predicci√≥n con LSTM\n",
        "        # ========================\n",
        "        for i in altas:\n",
        "            try:\n",
        "                serie = imfs_full[i].reshape(-1, 1)\n",
        "                scaler = MinMaxScaler()\n",
        "                serie_scaled = scaler.fit_transform(serie)\n",
        "\n",
        "                # √öltima ventana conocida\n",
        "                ultimos_valores = serie_scaled[-n_steps:]\n",
        "                model_path = os.path.join(DIRS[\"lstm_models\"], f\"{nombre_archivo}_imf{i}_lstm.keras\")\n",
        "                model = load_model(model_path)\n",
        "\n",
        "                pred = []\n",
        "                entrada = ultimos_valores.copy()\n",
        "\n",
        "                for _ in range(pasos):\n",
        "                    X_input = entrada.reshape((1, n_steps, 1))\n",
        "                    pred_scaled = model.predict(X_input, verbose=0)\n",
        "                    pred.append(pred_scaled[0][0])\n",
        "                    entrada = np.append(entrada[1:], pred_scaled).reshape(n_steps, 1)\n",
        "\n",
        "                pred_inv = scaler.inverse_transform(np.array(pred).reshape(-1, 1)).flatten()\n",
        "                pred_total += pred_inv\n",
        "            except Exception as e:\n",
        "                print(f\"‚ö†Ô∏è LSTM IMF{i+1} - {e}\")\n",
        "\n",
        "        # ========================\n",
        "        # Predicci√≥n del residual con ARIMA\n",
        "        # ========================\n",
        "        try:\n",
        "            residual = serie_full - imfs_full.sum(axis=0)\n",
        "            model_path = os.path.join(DIRS[\"arima_models\"], f\"{nombre_archivo}_residual_arima.pkl\")\n",
        "            with open(model_path, \"rb\") as f:\n",
        "                modelo = pickle.load(f)\n",
        "            pred = modelo.predict(start=len(modelo.model.endog), end=len(modelo.model.endog) + pasos - 1)\n",
        "            pred_total += pred\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Residual ARIMA - {e}\")\n",
        "\n",
        "        # ========================\n",
        "        # Construcci√≥n del DataFrame de resultados\n",
        "        # ========================\n",
        "        fecha_inicio = fechas_full[-1] + relativedelta(months=1)\n",
        "        fechas_futuras = pd.date_range(fecha_inicio, periods=pasos, freq='MS')\n",
        "        df_pred = pd.DataFrame({\"Fecha\": fechas_futuras, \"Prediccion\": pred_total})\n",
        "        predicciones_futuras[est] = df_pred\n",
        "\n",
        "    return predicciones_futuras\n"
      ],
      "metadata": {
        "id": "7g992809ozU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìä Generaci√≥n de tablas de predicci√≥n futura por estaci√≥n\n",
        "\n",
        "La funci√≥n `generar_tablas_predicciones(...)`:\n",
        "\n",
        "- Recibe:\n",
        "  - `df_original`: dataset original con datos hist√≥ricos reales.\n",
        "  - `pred_futuras`: diccionario con predicciones generadas por estaci√≥n.\n",
        "- Para cada estaci√≥n:\n",
        "  - Convierte las fechas futuras a columnas de mes (en espa√±ol) y a√±o.\n",
        "  - Crea una tabla estilo calendario (A√±o √ó Mes) con las predicciones.\n",
        "  - Calcula el **promedio hist√≥rico mensual** de cada estaci√≥n con base en los datos originales.\n",
        "  - A√±ade este promedio como una fila adicional de referencia.\n",
        "  - Muestra la tabla redondeada a 2 decimales.\n",
        "  - Guarda la tabla en un archivo `.csv` en la ruta `predicciones_futuras`.\n",
        "\n",
        "Estas tablas permiten una visualizaci√≥n clara y exportable de los resultados para informes t√©cnicos o toma de decisiones.\n"
      ],
      "metadata": {
        "id": "kD2MjFe9pDpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================================\n",
        "# Tablas anuales con predicciones mensuales por estaci√≥n\n",
        "# ==========================================================\n",
        "\n",
        "def generar_tablas_predicciones(df_original, pred_futuras):\n",
        "    meses_es = ['Enero','Febrero','Marzo','Abril','Mayo','Junio',\n",
        "                'Julio','Agosto','Septiembre','Octubre','Noviembre','Diciembre']\n",
        "\n",
        "    mapping = {\n",
        "        'January': 'Enero', 'February': 'Febrero', 'March': 'Marzo',\n",
        "        'April': 'Abril', 'May': 'Mayo', 'June': 'Junio',\n",
        "        'July': 'Julio', 'August': 'Agosto', 'September': 'Septiembre',\n",
        "        'October': 'Octubre', 'November': 'Noviembre', 'December': 'Diciembre'\n",
        "    }\n",
        "\n",
        "    for est, df_pred in pred_futuras.items():\n",
        "        # Extrae a√±o y mes en espa√±ol\n",
        "        df_pred[\"A√±o\"] = df_pred[\"Fecha\"].dt.year\n",
        "        df_pred[\"Mes\"] = df_pred[\"Fecha\"].dt.month_name().map(mapping)\n",
        "\n",
        "        # Reorganiza en tabla A√±o √ó Mes\n",
        "        tabla = df_pred.pivot_table(index=\"A√±o\", columns=\"Mes\", values=\"Prediccion\", aggfunc='sum')\n",
        "        tabla = tabla[[m for m in meses_es if m in tabla.columns]]\n",
        "\n",
        "        # Calcula promedio hist√≥rico mensual\n",
        "        promedio_mensual = (\n",
        "            df_original[df_original['NombreEstacion'] == est]\n",
        "            .assign(Mes=lambda x: x['Fecha'].dt.month_name().map(mapping))\n",
        "            .groupby(\"Mes\")['Valor'].mean()\n",
        "            .reindex(meses_es)\n",
        "        )\n",
        "\n",
        "        # Agrega el promedio hist√≥rico como fila\n",
        "        tabla.loc[\"Promedio hist√≥rico\"] = promedio_mensual.values\n",
        "\n",
        "        # Muestra la tabla en pantalla\n",
        "        display(tabla.round(2))\n",
        "\n",
        "        # Guarda como CSV\n",
        "        ruta = os.path.join(DIRS[\"predicciones_futuras\"], f\"{est.replace('/', '_')}_prediccion_futura.csv\")\n",
        "        tabla.to_csv(ruta)\n",
        "        print(f\"üìÅ CSV guardado: {ruta}\")\n"
      ],
      "metadata": {
        "id": "Nu5tgVaPpD51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ‚è© Ejecuci√≥n de predicci√≥n futura y generaci√≥n de tablas\n",
        "\n",
        "En esta celda se ejecuta:\n",
        "\n",
        "1. `generar_predicciones_futuras(...)`: proyecta la precipitaci√≥n mensual para los pr√≥ximos 35 meses por estaci√≥n, utilizando los modelos ARIMA y LSTM previamente entrenados.\n",
        "2. `generar_tablas_predicciones(...)`: organiza los resultados en tablas tipo calendario (A√±o √ó Mes) y agrega el promedio hist√≥rico mensual como referencia. Cada tabla se guarda en formato `.csv` para su uso posterior.\n",
        "\n",
        "Este paso final entrega un resumen cuantitativo claro de las predicciones futuras por estaci√≥n.\n"
      ],
      "metadata": {
        "id": "QEN1uL_FpPXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =====================================================\n",
        "# Ejecutar predicci√≥n futura (35 meses) y generar tablas\n",
        "# =====================================================\n",
        "\n",
        "pred_futuras = generar_predicciones_futuras(imfs_dict, stats_dict_train, n_steps=10, pasos=35)\n",
        "generar_tablas_predicciones(df, pred_futuras)\n"
      ],
      "metadata": {
        "id": "kJXfedzopP0b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}